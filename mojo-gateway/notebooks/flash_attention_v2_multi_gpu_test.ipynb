{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlashAttention-2 Multi-GPU Test\n",
    "\n",
    "Tests tensor parallel attention across multiple GPUs.\n",
    "\n",
    "**Note:** Colab free tier has 1 GPU. For true multi-GPU testing, use:\n",
    "- Colab Pro (A100 x2)\n",
    "- Kaggle (T4 x2)\n",
    "- Cloud VMs with multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Check available GPUs\n",
    "result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True)\n",
    "print(\"Available GPUs:\")\n",
    "print(result.stdout)\n",
    "\n",
    "# Count GPUs\n",
    "gpu_count = result.stdout.count('GPU ')\n",
    "print(f\"Total GPUs: {gpu_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NCCL (required for multi-GPU)\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq libnccl2 libnccl-dev\n",
    "print(\"NCCL installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!rm -rf ollama-api-gateway\n",
    "!git clone --depth 1 https://github.com/umerkhan95/ollama-api-gateway.git\n",
    "%cd ollama-api-gateway/mojo-gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Multi-GPU Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd src/kernels/cuda\n",
    "!make clean\n",
    "\n",
    "# Build FlashAttention-2 (single GPU)\n",
    "!make CUDA_ARCH=\"-gencode arch=compute_75,code=sm_75\" \\\n",
    "      NVCC_FLAGS_COMMON=\"-O3 -Xcompiler -fPIC -Xcompiler -Wall\" \\\n",
    "      fa2\n",
    "\n",
    "print(\"\\nSingle-GPU FA2 built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Multi-GPU FlashAttention-2\n",
    "!make CUDA_ARCH=\"-gencode arch=compute_75,code=sm_75\" \\\n",
    "      NVCC_FLAGS_COMMON=\"-O3 -Xcompiler -fPIC -Xcompiler -Wall\" \\\n",
    "      fa2-multi-gpu\n",
    "\n",
    "print(\"\\nMulti-GPU FA2 built!\")\n",
    "!ls -la ../../../lib/*.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compile and Run Multi-GPU Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile test binary\n",
    "!nvcc -O3 -gencode arch=compute_75,code=sm_75 \\\n",
    "    -o test_multi_gpu test_flash_attention_v2_multi_gpu.cu \\\n",
    "    flash_attention_v2.o flash_attention_v2_multi_gpu.o \\\n",
    "    -lnccl -lpthread -lcudart\n",
    "\n",
    "print(\"Test binary compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multi-GPU benchmark\n",
    "!./test_multi_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Single GPU Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single-GPU vs multi-GPU (if multiple GPUs available)\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True)\n",
    "gpu_count = result.stdout.count('GPU ')\n",
    "\n",
    "if gpu_count == 1:\n",
    "    print(\"=\"*60)\n",
    "    print(\"  SINGLE GPU ENVIRONMENT DETECTED\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    print(\"Multi-GPU tensor parallelism cannot be fully tested.\")\n",
    "    print(\"The test above validates:\")\n",
    "    print(\"  - Code compiles with NCCL\")\n",
    "    print(\"  - Single-GPU initialization works\")\n",
    "    print(\"  - API functions are callable\")\n",
    "    print()\n",
    "    print(\"For true multi-GPU testing, use:\")\n",
    "    print(\"  - Colab Pro (A100 x2)\")\n",
    "    print(\"  - Kaggle (T4 x2)\")\n",
    "    print(\"  - AWS p3.8xlarge (V100 x4)\")\n",
    "    print(\"  - GCP a2-highgpu-4g (A100 x4)\")\n",
    "else:\n",
    "    print(f\"Multi-GPU environment: {gpu_count} GPUs\")\n",
    "    print(\"Full tensor parallel testing available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Expected Performance\n",
    "\n",
    "Based on tensor parallelism theory and NCCL overhead:\n",
    "\n",
    "| GPUs | Speedup | Throughput (est.) |\n",
    "|------|---------|-------------------|\n",
    "| 1    | 1.0x    | 708 tok/s         |\n",
    "| 2    | 1.7-2.0x| 1200-1400 tok/s   |\n",
    "| 4    | 2.5-3.1x| 1770-2200 tok/s   |\n",
    "| 8    | 3.5-4.0x| 2500-2800 tok/s   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "!rm -f test_multi_gpu\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
