{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EdgeLLM CUDA Inference Test\n",
    "\n",
    "This notebook tests the CUDA T-MAC kernels for GPU-accelerated BitNet inference.\n",
    "\n",
    "**Requirements:**\n",
    "- NVIDIA GPU (Jetson, RTX, etc.)\n",
    "- CUDA Toolkit 11.0+\n",
    "- nvcc compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NVIDIA GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA version\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU details\n",
    "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository and Build CUDA Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (if not already present)\n",
    "import os\n",
    "if not os.path.exists('ollama-api-gateway'):\n",
    "    !git clone https://github.com/umerkhan95/ollama-api-gateway.git\n",
    "else:\n",
    "    print('Repository already exists, pulling latest changes...')\n",
    "    !cd ollama-api-gateway && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to kernels directory\n",
    "%cd ollama-api-gateway/mojo-gateway/src/kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CUDA kernels\n",
    "!make cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify build output\n",
    "!ls -la ../../lib/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run CUDA Kernel Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CUDA unit tests\n",
    "!make cuda-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Python CUDA Kernel Test\n",
    "\n",
    "Test the CUDA kernels directly from Python using ctypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Find the CUDA library\n",
    "lib_path = '../../lib/libtmac_kernel_cuda.so'\n",
    "if not os.path.exists(lib_path):\n",
    "    raise FileNotFoundError(f'CUDA library not found at {lib_path}. Run make cuda first.')\n",
    "\n",
    "# Load the library\n",
    "cuda_lib = ctypes.CDLL(lib_path)\n",
    "print(f'Loaded CUDA library: {lib_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function signatures\n",
    "cuda_lib.cuda_available.restype = ctypes.c_int\n",
    "cuda_lib.cuda_device_name.restype = ctypes.c_char_p\n",
    "cuda_lib.cuda_init.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int]\n",
    "cuda_lib.cuda_init.restype = ctypes.c_int\n",
    "cuda_lib.cuda_cleanup.restype = None\n",
    "\n",
    "# Check CUDA availability\n",
    "if cuda_lib.cuda_available():\n",
    "    device_name = cuda_lib.cuda_device_name().decode('utf-8')\n",
    "    print(f'CUDA Available: Yes')\n",
    "    print(f'Device: {device_name}')\n",
    "else:\n",
    "    print('CUDA Not Available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CUDA\n",
    "max_weights = 10_000_000  # 10MB\n",
    "max_activations = 1_000_000\n",
    "max_output = 1_000_000\n",
    "\n",
    "ret = cuda_lib.cuda_init(max_weights, max_activations, max_output)\n",
    "if ret == 0:\n",
    "    print('CUDA initialized successfully')\n",
    "else:\n",
    "    print('CUDA initialization failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RMSNorm kernel\n",
    "cuda_lib.rmsnorm_cuda.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float),  # output\n",
    "    ctypes.POINTER(ctypes.c_float),  # input\n",
    "    ctypes.POINTER(ctypes.c_float),  # weight\n",
    "    ctypes.c_int,                     # batch_size\n",
    "    ctypes.c_int,                     # size\n",
    "    ctypes.c_float                    # eps\n",
    "]\n",
    "cuda_lib.rmsnorm_cuda.restype = ctypes.c_int\n",
    "\n",
    "# Create test data\n",
    "batch_size = 4\n",
    "size = 256\n",
    "\n",
    "input_data = np.random.randn(batch_size, size).astype(np.float32)\n",
    "weight_data = np.ones(size, dtype=np.float32)\n",
    "output_data = np.zeros((batch_size, size), dtype=np.float32)\n",
    "\n",
    "# Run RMSNorm on GPU\n",
    "ret = cuda_lib.rmsnorm_cuda(\n",
    "    output_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "    input_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "    weight_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "    batch_size,\n",
    "    size,\n",
    "    ctypes.c_float(1e-6)\n",
    ")\n",
    "\n",
    "if ret == 0:\n",
    "    print('RMSNorm CUDA: SUCCESS')\n",
    "    print(f'Input mean: {input_data.mean():.4f}')\n",
    "    print(f'Output mean: {output_data.mean():.4f}')\n",
    "    print(f'Output std: {output_data.std():.4f}')\n",
    "else:\n",
    "    print('RMSNorm CUDA: FAILED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Softmax kernel\n",
    "cuda_lib.softmax_cuda.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float),  # output\n",
    "    ctypes.POINTER(ctypes.c_float),  # input\n",
    "    ctypes.c_int,                     # batch_size\n",
    "    ctypes.c_int                      # size\n",
    "]\n",
    "cuda_lib.softmax_cuda.restype = ctypes.c_int\n",
    "\n",
    "# Create test data\n",
    "logits = np.random.randn(batch_size, size).astype(np.float32) * 2\n",
    "probs = np.zeros((batch_size, size), dtype=np.float32)\n",
    "\n",
    "# Run Softmax on GPU\n",
    "ret = cuda_lib.softmax_cuda(\n",
    "    probs.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "    logits.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "    batch_size,\n",
    "    size\n",
    ")\n",
    "\n",
    "if ret == 0:\n",
    "    print('Softmax CUDA: SUCCESS')\n",
    "    # Verify softmax sums to 1\n",
    "    for b in range(batch_size):\n",
    "        row_sum = probs[b].sum()\n",
    "        print(f'  Batch {b} sum: {row_sum:.6f} (should be ~1.0)')\n",
    "else:\n",
    "    print('Softmax CUDA: FAILED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark RMSNorm\n",
    "batch_size = 32\n",
    "size = 4096  # Typical hidden size\n",
    "iterations = 1000\n",
    "\n",
    "input_data = np.random.randn(batch_size, size).astype(np.float32)\n",
    "weight_data = np.ones(size, dtype=np.float32)\n",
    "output_data = np.zeros((batch_size, size), dtype=np.float32)\n",
    "\n",
    "# Warmup\n",
    "for _ in range(10):\n",
    "    cuda_lib.rmsnorm_cuda(\n",
    "        output_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "        input_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "        weight_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "        batch_size, size, ctypes.c_float(1e-6)\n",
    "    )\n",
    "\n",
    "# Benchmark\n",
    "cuda_lib.cuda_sync()  # Ensure warmup is done\n",
    "start = time.perf_counter()\n",
    "for _ in range(iterations):\n",
    "    cuda_lib.rmsnorm_cuda(\n",
    "        output_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "        input_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "        weight_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "        batch_size, size, ctypes.c_float(1e-6)\n",
    "    )\n",
    "cuda_lib.cuda_sync()\n",
    "end = time.perf_counter()\n",
    "\n",
    "total_time = end - start\n",
    "per_call = total_time / iterations * 1000  # ms\n",
    "throughput = iterations / total_time\n",
    "\n",
    "print(f'RMSNorm Benchmark ({batch_size}x{size}):')\n",
    "print(f'  Total time: {total_time:.3f}s for {iterations} iterations')\n",
    "print(f'  Per call: {per_call:.3f}ms')\n",
    "print(f'  Throughput: {throughput:.1f} calls/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Softmax\n",
    "logits = np.random.randn(batch_size, size).astype(np.float32)\n",
    "probs = np.zeros((batch_size, size), dtype=np.float32)\n",
    "\n",
    "# Warmup\n",
    "for _ in range(10):\n",
    "    cuda_lib.softmax_cuda(\n",
    "        probs.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "        logits.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "        batch_size, size\n",
    "    )\n",
    "\n",
    "# Benchmark\n",
    "cuda_lib.cuda_sync()\n",
    "start = time.perf_counter()\n",
    "for _ in range(iterations):\n",
    "    cuda_lib.softmax_cuda(\n",
    "        probs.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "        logits.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
    "        batch_size, size\n",
    "    )\n",
    "cuda_lib.cuda_sync()\n",
    "end = time.perf_counter()\n",
    "\n",
    "total_time = end - start\n",
    "per_call = total_time / iterations * 1000\n",
    "throughput = iterations / total_time\n",
    "\n",
    "print(f'Softmax Benchmark ({batch_size}x{size}):')\n",
    "print(f'  Total time: {total_time:.3f}s for {iterations} iterations')\n",
    "print(f'  Per call: {per_call:.3f}ms')\n",
    "print(f'  Throughput: {throughput:.1f} calls/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "cuda_lib.cuda_cleanup()\n",
    "print('CUDA resources cleaned up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Full T-MAC MatMul Benchmark\n\nTest the T-MAC matmul kernel that eliminates LUT rebuilding overhead."
  },
  {
   "cell_type": "code",
   "source": "# Re-initialize CUDA\nret = cuda_lib.cuda_init(60_000_000, 4096, 50000)  # 60MB for weights\nprint(f'CUDA re-initialized: {ret == 0}')\n\n# Define T-MAC matmul signature\ncuda_lib.tmac_matmul_cuda.argtypes = [\n    ctypes.POINTER(ctypes.c_int8),    # weights (packed ternary)\n    ctypes.POINTER(ctypes.c_float),   # activations\n    ctypes.POINTER(ctypes.c_float),   # output\n    ctypes.POINTER(ctypes.c_float),   # scales\n    ctypes.c_int,                      # M (output rows)\n    ctypes.c_int,                      # N (batch size)\n    ctypes.c_int                       # K (input dimension)\n]\ncuda_lib.tmac_matmul_cuda.restype = ctypes.c_int\n\n# SmolLM-135M dimensions\ndim = 576        # hidden dimension\nhidden_dim = 1536  # FFN intermediate dimension\nvocab_size = 49152  # vocabulary size\n\nprint(f'\\nSmolLM-135M dimensions:')\nprint(f'  dim (hidden): {dim}')\nprint(f'  hidden_dim (FFN): {hidden_dim}')\nprint(f'  vocab_size: {vocab_size}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create test data for T-MAC matmul\nM = dim  # Output rows (e.g., QKV projection)\nN = 1    # Batch size (single token)\nK = dim  # Input dimension\n\n# Packed ternary weights: each byte holds 4 ternary values\nbytes_per_row = (K + 3) // 4\nweights = np.random.randint(-128, 127, (M, bytes_per_row), dtype=np.int8)\nactivations = np.random.randn(K).astype(np.float32)\noutput = np.zeros(M, dtype=np.float32)\nscales = np.random.uniform(0.1, 1.0, M).astype(np.float32)\n\nprint(f'T-MAC MatMul dimensions:')\nprint(f'  M={M}, N={N}, K={K}')\nprint(f'  Weights: {weights.shape} ({weights.nbytes / 1024:.1f} KB)')\nprint(f'  Activations: {activations.shape}')\nprint(f'  Output: {output.shape}')\n\n# Run T-MAC matmul\nret = cuda_lib.tmac_matmul_cuda(\n    weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8)),\n    activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n    output.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n    scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n    M, N, K\n)\n\nprint(f'\\nT-MAC MatMul result: {\"SUCCESS\" if ret == 0 else \"FAILED\"}')\nprint(f'Output mean: {output.mean():.4f}, std: {output.std():.4f}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Benchmark per-layer operations (simulating full forward pass)\niterations = 100\n\n# Layer operations for SmolLM-135M:\n# - QKV projection: (dim, dim) x 3\n# - Output projection: (dim, dim)\n# - FFN gate/up: (hidden_dim, dim) x 2\n# - FFN down: (dim, hidden_dim)\n# Total per layer: 7 matmuls\n\nlayers = [\n    ('QKV Projection', dim, dim),\n    ('Output Projection', dim, dim),\n    ('FFN Gate', hidden_dim, dim),\n    ('FFN Up', hidden_dim, dim),\n    ('FFN Down', dim, hidden_dim),\n]\n\nprint('Per-Layer Benchmark (SmolLM-135M, single token):')\nprint('=' * 60)\n\ntotal_time_per_layer = 0\n\nfor name, M, K in layers:\n    bytes_per_row = (K + 3) // 4\n    weights = np.random.randint(-128, 127, (M, bytes_per_row), dtype=np.int8)\n    activations = np.random.randn(K).astype(np.float32)\n    output = np.zeros(M, dtype=np.float32)\n    scales = np.random.uniform(0.1, 1.0, M).astype(np.float32)\n    \n    # Warmup\n    for _ in range(10):\n        cuda_lib.tmac_matmul_cuda(\n            weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8)),\n            activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n            output.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n            scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n            M, 1, K\n        )\n    \n    cuda_lib.cuda_sync()\n    start = time.perf_counter()\n    for _ in range(iterations):\n        cuda_lib.tmac_matmul_cuda(\n            weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8)),\n            activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n            output.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n            scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n            M, 1, K\n        )\n    cuda_lib.cuda_sync()\n    end = time.perf_counter()\n    \n    per_call_ms = (end - start) / iterations * 1000\n    total_time_per_layer += per_call_ms\n    \n    print(f'{name:20s} ({M:4d} x {K:4d}): {per_call_ms:.3f} ms')\n\nprint('=' * 60)\nprint(f'Total per layer: {total_time_per_layer:.3f} ms')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Estimate full model throughput\nn_layers = 30  # SmolLM-135M has 30 transformer layers\n\n# Per-token overhead breakdown\nmatmul_time_per_token = total_time_per_layer * n_layers  # 7 matmuls * 30 layers\nattention_time_estimate = 0.5  # ms (depends on sequence length)\nother_overhead = 0.5  # ms (RoPE, residuals, sampling)\n\ntotal_per_token = matmul_time_per_token + attention_time_estimate + other_overhead\nestimated_throughput = 1000 / total_per_token  # tokens per second\n\nprint('\\n' + '=' * 60)\nprint('ESTIMATED FULL MODEL THROUGHPUT')\nprint('=' * 60)\nprint(f'Model: SmolLM-135M ({n_layers} layers)')\nprint(f'')\nprint(f'Per-token breakdown:')\nprint(f'  MatMul ({n_layers} layers x {len(layers)} ops): {matmul_time_per_token:.2f} ms')\nprint(f'  Attention estimate:              {attention_time_estimate:.2f} ms')\nprint(f'  Other overhead:                  {other_overhead:.2f} ms')\nprint(f'  -----------------------------------')\nprint(f'  Total per token:                 {total_per_token:.2f} ms')\nprint(f'')\nprint(f'ESTIMATED THROUGHPUT: {estimated_throughput:.1f} tok/s')\nprint('')\nprint('Comparison:')\nprint(f'  Ollama (baseline):    423 tok/s')\nprint(f'  EdgeLLM CUDA:         {estimated_throughput:.1f} tok/s')\nprint(f'  Speedup:              {estimated_throughput/423:.2f}x')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cleanup\ncuda_lib.cuda_cleanup()\nprint('CUDA resources cleaned up')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Summary\n\nThis notebook tested the CUDA T-MAC kernel integration:\n\n### What We Tested\n1. **CUDA Environment** - GPU detection and initialization\n2. **RMSNorm Kernel** - GPU-accelerated layer normalization\n3. **Softmax Kernel** - GPU-accelerated attention softmax\n4. **T-MAC MatMul** - Ternary weight matrix multiplication (NO LUT rebuilding!)\n\n### Key Improvement: Eliminated LUT Rebuilding Overhead\n\n**Before (CPU path):**\n- Built 150 LUTs per token (5 per layer Ã— 30 layers)\n- Each LUT: ~36K-98K float operations\n- Total: ~7 million float ops just for LUT building per token!\n\n**After (CUDA path):**\n- CUDA kernel builds LUT internally once per matmul\n- GPU parallelism handles LUT + matmul in single operation\n- No separate LUT build step\n\n### Next Steps\n1. Test with real SmolLM-135M model weights\n2. Run full Mojo inference with CUDA backend\n3. Compare long-running (100+ tokens) throughput vs Ollama",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}